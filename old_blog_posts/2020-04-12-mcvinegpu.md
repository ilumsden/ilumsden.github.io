---
layout: post
title: ORNL Internship&#58; McVineGPU
categories: [Research, Reflections]
---

During the second summer and fall semester of my ORNL internship, I worked on my final and possibly most important project. Like my previous [SCADGen project]({% post_url 2020-04-12-scadgen %}), this project was connected to MCViNE, a neutron scattering simulator. However, unlike SCADGen, this project had me working directly with MCViNE's source code. At a high level, the goal of my project was simple: rewrite several core pieces of code in MCViNE to use Graphics Processing Units (GPUs) instead of Central Processing Units (CPUs). 

Although this doesn't sound terribly complicated, this was actually one of the toughest projects I have worked on yet, mainly due to the differences between CPUs and GPUs. CPUs are inherantly sequential processors. Each CPU core (the actual processing circuits) is a Single-Instruction, Single Data (SISD) processor. This means that, at any given time, a CPU core can perform a single basic task on either one or two single elements of data (i.e. adding two numbers). However, to keep the overall CPU package from being too slow, modern "CPUs" actually contain several CPU cores to allow it to process multiple instructions on multiple data elements (MIMD). The end result of this SISD/MIMD design is that CPUs are good at performing relatively complex computational tasks (such as conditional logic) sequentially. On the other hand, GPUs are Single-Instruction, Multi-Data (SIMD, or SIMT if you want to use NVIDIA's terminology) processors. Another name for this type of processor is a "vector processor". This is because GPU cores perform the same operation on a collection, or vector, of data at the same time (in parallel)(i.e. adding two vectors of numbers). The end result of this SIMD/T design is that GPUs are best at performing relatively simple operations across data in parallel. An example of the differences between CPUs and GPUs is adding two 1D collections (vectors) of numbers. To perform this task, a CPU would have to iteratively step through each corresponding pair of numbers in each vector, performing an addition on each pair. If an addition operation takes time _t_, for a 10 element vector, this operation would take 10*_t_. On the other hand, a GPU could perform all 10 additions at the same time (in parallel). Thus, the time it would take to perform this operation on a GPU is just _t_.

The innate parallelism of GPUs was probably the hardest challenge for me to overcome in this project. Today, Computer Science education and most internships focus on "traditional" CPU-based coding, which is innately sequential. When you reason about CPU code, you can think of it as an iterative series of commands. However, when programming on a GPU, your reasoning must be innately parallel. You can think about each parallel "thread" sequentially, but you also have to consider the way in which these threads interact, both in terms of code flow and in terms of memory. If you don't, your entire program will fall out of sync, and you will almost certainly end up corrupting data somewhere on your computer (the operating system usually protects you from corrupting anything too important). Thus, a large part of this project was learning to conceptualize and reason about code in this new, parallel way. As part of that, I also had to learn how to program in NVIDIA's CUDA framework, which, up until the end of 2018, was pretty much the only way to write code dedicated for GPUs.

Once I learned these things, I had to apply them to several parts of the MCViNE code. By the end of the project, I had managed to rewrite each phase of the simulation for simple geometries (i.e. cubes, spheres, pyramids, etc.). Just like in the CPU version, I based my code on a ray tracing strategy. Ray tracing is a process in which various physics- and geometry-based calculations are applied to a particle to simulate their realistic motion. It is mostly commonly heard of today because of the push for ray tracing-based computer graphics for video games. In MCViNE, the general steps in the simulation are as follows. First, each neutron is considered to be a ray, meaning it consists of a position and velocity. Using Newton's laws of motion and 3D geometry, the entry and exit points of the neutrons through the sample (what's being exposed to the neutron beam) are calculated. Then, using quantum mechanics-based probability, a "scattering" point is calculated. This point represents the spot where the neutron is redirected by the atomic structure of the sample. Again using quantum mechanics-based probability, a new velocity is calculated for the neutron and the process repeats until the neutron's position matches that of a predefined "detector". My GPU-based version of this process is very similar, with only two differences. The first and biggest difference is that the quantum mechanics-based probability was not implemented, mainly because I simply ran out of time. Instead, this was replaced with a random number generator. The other difference is in how this is run on hardware. In the original MCViNE, each phase of this process would have to be performed once for each neutron before moving on to the next step. That means, if there were 100000 neutrons, each phase would be performed 100000 times before moving on to the next step. In my GPU-based version, the GPU was used to perform all neutron calculations in parallel. That means that each phase would be performed on all 100000 neutrons at roughly the same time. All intensive calculations were performed on the GPU, with the CPU only being used to introduce data to the GPU and link GPU routines together.

In the end, this project was a huge success, with my GPU-based code achieving a 200-time speedup compared to the original MCViNE. Granted this was in a rather simple test (passing 100000 neutrons through a box and only allowing a single scattering event per neutron), but it was still quite promising. However, there was one drawback, and it's the same drawback that has caused issues with ray tracing graphics for years: memory. All processors need memory to store the data that they operate on. However, CPUs and GPUs use two different types of memory, which requires them to have their own dedicated memory on a system. For CPUs, this isn't an issue, as memory is usually quite configurable. However, on GPUs, the memory is usually directly soldered onto the circuit board. This means that, even on the most expensive server grade GPUs, you will usually only have about 32 GB of memory per processor. On the GPUs that I was using, there were only 16 GB of memory, and, if I tried simulating much more than the 100000 neutrons that I used in my tests, I would actually run out of memory of the GPU. Thus, my GPU-based version of MCViNE was capped at roughly 100000 neutrons per simulation, which, to a neutron physicist, is almost nothing. Towards the end of my time on this project, I tried to mitigate this issue by adding the ability to use multiple GPUs. However, I was unable to complete this feature in time.

Along with being possibly my most challenging project yet, this project was also one of the most influential. While working on it, I realized I really enjoyed the challenge associated with GPU computing and parallel computing in general. My internship with ORNL ended with this project at the end of 2018, but, when I began looking for a new job for the summer of 2019, my enjoyment of this project ended up playing a major factor in my decision. While looking for jobs, I decided to consider performing research within the Electrical Engineering and Computer Science Department here at UT. I considered many professors to approach, but, in the end, my enjoyment of parallel computing led me to first reach out to Dr. Michela Taufer, whose group performs research in High-Performance Computing. Although I was planning to reach out to several professors before making a decision, I quickly became excited and enamored by the work Dr. Taufer's Global Computing Lab was doing, and I quickly joined for the summer. This summer position in the lab quickly led to a part-time Undergraduate Research Assistantship during this 2019-2020 school year, which quickly expanded into me applying for the PhD program here at UT with Dr. Taufer as my advisor. This is why my last ORNL project was the most influential on me. It exposed me to a new subfield of Computer Science that I likely wouldn't have otherwise experienced, and, in doing so, this project has helped dramatically change the path of my career for the better.

<p>
<img src="/static/assets/img/blog/posts/orise_poster_session.jpg" style="max-width: 100%; max-height: 100vh; margin: auto;" alt="">
<div style="width: 100%; text-align: center;">
<i>ORISE Annual Summer Undergrad Intern Poster Session</i>
</div>
</p>

_I made a poster for the McVineGPU project for ORNL annual summer internship poster session. If you want to see it, click [here](https://drive.google.com/file/d/1_djBKIlLmSXF0GgSH6YhpNTF0bfVZfSo/view?usp=sharing)._
